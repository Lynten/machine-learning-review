{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归（Logistic Regression）\n",
    "\n",
    "逻辑回归是机器学习中较为简单的分类算法，可以用于二元分类预测，和线性回归不同点在于，线性回归预测的结果是连续型，而逻辑回归预测的结果是离散型。随着智慧医疗的发展，越来越多的机器学习方法模型被用于辅助医生决策，其中逻辑回归就是其中常用的一种。比如，根据一个人的体重和吸烟量预测他是否会得癌症的概率；根据一个人的年龄、卡路里摄入量、脂肪摄入量预测他是否会得心脏病的概率。\n",
    "\n",
    "逻辑回归的假设函数为：\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x})=\\frac{1}{1+e^{-\\theta \\mathbf{x}}}\n",
    "$$\n",
    "\n",
    "可以看到，逻辑回归的假设函数是在线性回归的基础上套了一个Sigmoid函数：\n",
    "\n",
    "$$\n",
    "\\sigma(x)=\\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "这样我们就可以根据$h(\\mathbf{x})$的值判断预测结果，当大于0.5时为正例，小于等于0.5时为负例。\n",
    "\n",
    "Sigmoid函数具有很好的性质，比如将实数域映射到了(0,1)，另外Sigmoid函数的导数可以用自身来表示。\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\sigma'(x) = & -(1+e^{-x})^{-2} \\cdot e^{-x} \\cdot -1 \\\\\n",
    "= & \\frac{e^{-x}}{(1+e^{-x})^{2}} \\\\\n",
    "= & \\frac{e^{-x}+1-1}{(1+e^{-x})^{2}} \\\\\n",
    "= & \\sigma(x)(1-\\sigma(x))\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "也因此可得(这里的对数均为自然对数)：\n",
    "\n",
    "$$\\log(\\sigma(x))=\\frac{1}{\\sigma(x)}\\cdot\\sigma(x)(1-\\sigma(x))=1-\\sigma(x)$$\n",
    "$$\\log(1-\\sigma(x))=\\frac{1}{1-\\sigma(x)} \\cdot -1 \\cdot \\sigma(x)(1-\\sigma(x))=-\\sigma(x)$$\n",
    "\n",
    "这两个式子在梯度下降法求最优解时会用到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数：\n",
    "$$\n",
    "J(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}(y^i\\log h(\\mathbf{x}^i)+(1-y^i)\\log(1-h(\\mathbf{x}^i)))\n",
    "$$\n",
    "\n",
    "当$y^i=1$时，损失函数只有$\\log h(\\mathbf{x}^i)$这一项，当$h(\\mathbf{x}^i)$趋向于0时，其值趋向于$-\\infty$。\n",
    "\n",
    "当$y^i=0$时，损失函数只有$\\log(1-h(\\mathbf{x}^i))$这一项，当$h(\\mathbf{x}^i)$趋向于1时，其值趋向于$-\\infty$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和线性回归不同，逻辑回归的损失函数是求最大值，所以使用梯度上升法：\n",
    "\n",
    "Repeat {\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\theta_0 & = \\theta_0 +\\alpha \\frac{1}{n}\\sum_{i=1}^{n}(y^i-h(\\mathbf{x}^i))\\mathbf{x}^i_0 \\\\\n",
    "\\theta_1 & = \\theta_1 +\\alpha \\frac{1}{n}\\sum_{i=1}^{n}(y^i-h(\\mathbf{x}^i))\\mathbf{x}^i_1 \\\\\n",
    "\\theta_2 & = \\theta_2 +\\alpha \\frac{1}{n}\\sum_{i=1}^{n}(y^i-h(\\mathbf{x}^i))\\mathbf{x}^i_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_m & = \\theta_m +\\alpha \\frac{1}{n}\\sum_{i=1}^{n}(y^i-h(\\mathbf{x}^i))\\mathbf{x}^i_m \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "(update simultaneously)\n",
    "\n",
    "} until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述梯度下降法关键的步骤是$Z=y\\log h(\\mathbf{x})+(1-y)\\log(1-h(\\mathbf{x}))$对$\\theta$的偏导：\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{Z}}{\\partial{\\theta_1}}= & y \\cdot (1-h(\\mathbf{x}))\\cdot \\mathbf{x}_1+ (1-y) \\cdot -h(x) \\cdot \\mathbf{x}_1 \\\\\n",
    "= & y\\mathbf{x}_1 - yh(\\mathbf{x})\\mathbf{x}_1-(h(\\mathbf{x})x_1-yh(\\mathbf{x})\\mathbf{x}_1) \\\\\n",
    "= & (y-h(\\mathbf{x}))\\mathbf{x}_1\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
